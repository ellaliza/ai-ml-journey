{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d9c8ab",
   "metadata": {},
   "source": [
    "# Linear Models For Regression and Classification\n",
    "1. Simple linear regression using Ordinary Least Squares\n",
    "2. Gradient Descent Algorithm\n",
    "3. Regularized Regression Methods - Ridge, Lasso, ElasticNet\n",
    "4. Logistic Regression for Classification\n",
    "5. OnLine Learning Methods - Stochastic Graddient Descent and Passive Aggresive\n",
    "6. Robust Regresson - Dealing with ouliers and Model Errors\n",
    "7. Polynomial Regression\n",
    "8. Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13183fa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f971152f",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Using Ordinary Least Squares\n",
    "\n",
    "### Linearly Seperable Data\n",
    "> Two classes of data are linearly separable if you can draw a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions) that \n",
    "> completely separates the classes — with all points of one class on one side, and all points of the other on the other side.\n",
    "\n",
    "### Mathematical Aspect\n",
    "- We want to learn a function (a mathematical rule) that connects inputs to outputs using examples. These examples are what we call the training set.\n",
    "\n",
    "1. What is a training set?\n",
    "    For instance, when trying to figure out how someone's salary depends on their years of experience.\n",
    "\n",
    "    I'd collect data like this:\n",
    "\n",
    "    Years of Experience $(x^t)$\t| Salary $(r^t)$\n",
    "    -------------------|---------------------\n",
    "    1 | 30,000\n",
    "    2 | 35,000\n",
    "    3 |\t42,000\n",
    "\n",
    "\n",
    "    We call each row a training example.\n",
    "\n",
    "    $x^t$: the input (features or data point) at time or index t\n",
    "\n",
    "    $r^t$: the output (label, target value) at that index t\n",
    "\n",
    "    We call the full training set:\n",
    "\n",
    "    $X={(x^t ,r^t)}_{t=1}^N$\n",
    " \n",
    "    → This just means: a set of N input-output pairs.\n",
    "\n",
    "2.  __Goal__: Find a Function\n",
    "    We want to find a function \n",
    "\n",
    "    $f(x)$ that maps any input $x$ to the correct output $r$, based on the examples in the training set.\n",
    "\n",
    "    If there's no noise, that means the data is perfectly clean and consistent. So we have:\n",
    "\n",
    "    $r^t = f(x^t)$\n",
    "    This is called __interpolation__ — we find a function that goes exactly through all the points in our dataset.\n",
    "\n",
    "3. __Polynomial Interpolation__\n",
    "    If we have $N$ data points, we can fit a polynomial of degree N−1 that passes through all of them.\n",
    "\n",
    "    For example:\n",
    "\n",
    "        With 3 points → 2nd-degree polynomial (a parabola)\n",
    "\n",
    "        With 4 points → 3rd-degree polynomial, and so on.\n",
    "\n",
    "    This is helpful for predicting values within the range of known inputs.\n",
    "\n",
    "    But if we try to predict for an input outside the range of what we’ve seen before, it's called:\n",
    "\n",
    "4. __Extrapolation__\n",
    "    Predicting for values outside the range of training data (e.g., future values in a time series).\n",
    "\n",
    "5. __Why is There Noise?__\n",
    "    Because there are often hidden variables (things we don’t observe) that also affect the output. So the true function might actually be:\n",
    "\n",
    "    $r^t =f^∗ (x^t ,z^t)$  is the real function.\n",
    "\n",
    "    $z^t$ are the hidden factors we don’t see — like mood, weather, or other unknowns.\n",
    "\n",
    "    Since we can’t observe $z^t$ , we just try to find a good approximation using what we do know: $x$.\n",
    "\n",
    "6. __The Model: g(x)__  \n",
    "    We build a machine learning model $g(x)$ to approximate the true function $f(x)$. Our model is not perfect, but we want it to be as close as possible to the true outputs.\n",
    "\n",
    "7. __Measuring How Good Our Model Is__\n",
    "    We use Empirical Error (also called Mean Squared Error) to measure how well our model $g(x)$ does on the training set $X$.\n",
    "    E(g|X) = \\frac{1}{N}\\sum^N_{t=1}[r^t-g(x^t)]^2\n",
    "    - $r^t$ is the actual value.\n",
    "    - $g(x^t) is the model's prediction.\n",
    "    - We square the difference to make all errors positive.\n",
    "    - Then we average it over all examples.\n",
    "    - The smaller this value, the better the model fits the training data.\n",
    "\n",
    "### Components of a Linear Model\n",
    "\n",
    "1. **Features (Independent Variables)**\n",
    "    * Features are the input variables used to predict the target outcome. In **simple linear regression**, there's only one feature (\\(X\\)), but in **multiple linear regression**, there are several ($X_1, X_2, X_3,\\dots$). Features can be:\n",
    "        - **Numerical** (e.g., age, salary)\n",
    "        - **Categorical** (e.g., gender, product type—often one-hot encoded)\n",
    "        - **Derived or engineered** (e.g., interaction terms, polynomial features)\n",
    "\n",
    "2. **Weights (Coefficients)**\n",
    "    * Weights ($\\beta$) determine how much influence each feature has on the prediction. The model learns these weights during training using **optimization techniques** like **Ordinary Least Squares (OLS)** or **Gradient Descent**.\n",
    "\n",
    "    * For multiple linear regression:\n",
    "    * $ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_nX_n + \\epsilon $\n",
    "    * Each $\\beta$ represents how much the target $Y$ changes with a unit increase in its corresponding feature, assuming other features remain constant.\n",
    "\n",
    "3. **Bias (Intercept)**\n",
    "    * The intercept $\\beta_0$ is the value of $Y$ when all features $X_i$ are zero. It's crucial in adjusting the model's baseline prediction.\n",
    "\n",
    "4. **Error Term ($\\epsilon$)**\n",
    "    * This represents the part of $Y$ that **cannot** be explained by the features. Ideally, it should be normally distributed and independent.\n",
    "\n",
    "5. **Regularization (Preventing Overfitting)**\n",
    "    * In complex models, regularization is used to control weights and avoid overfitting:\n",
    "    * **Lasso Regression ($\\ell_1$ penalty)** shrinks some weights to zero, effectively performing feature selection.\n",
    "    * **Ridge Regression ($\\ell_2$ penalty)** penalizes large weights to encourage simpler models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc5f34",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bcd2887",
   "metadata": {},
   "source": [
    ":happy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d861b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
